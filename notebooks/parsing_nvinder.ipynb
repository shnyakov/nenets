{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Сбор данных на ненецком и русском языках с сайта nvinder.ru**"
   ],
   "metadata": {
    "id": "QkW5rwYq7s4v"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Постановка задачи**\n",
    "\n",
    "Для реализации модели нейронного машинного перевода и ее обучения потребуется создать собственный многоязычный параллельный корпус, так как существующие корпусы и удовлетворяющие ресурсы и материалы, связанные с ненецким языком, в сети Интернет не существуют или практически отсутствуют. Примером этого служит [опыт сбора](https://elibrary.ru/item.asp?id=65600587) текстовых данных в процессе разработки онлайн-словаря для реализации частотного словаря слов с целью точного определения объема релевантных слов в базе данных, в котором выявилось, что лингвистические материалы на ненецком языке представляют собой сложность в поиске и доступе.\n",
    "\n",
    "К таким материалам можно отнести малосодержательные посты и комментарии в социальной сети «ВКонтакте» и достаточно крупное для НАО СМИ – общественно-политическая [газета](https://nvinder.ru/) НАО «Няръяна вындер». Впрочем, на странице сайта газеты «Ялумд’’» часто можно найти статьи на русском языке, чем на ненецком.\n",
    "\n",
    "Помимо этого, существует большая вероятность смешать два ненецкого языка: лесной и тундровый. Настоящая работа ориентирована на литературный ненецкий язык, а именно на большеземельский говор тундрового ненецкого языка, что способствует разработке онлайн-словаря, который требуется на рынке для его сохранения и ревитализации."
   ],
   "metadata": {
    "id": "KHgXGgho77iL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Функция для получения HTML-кода страницы\n",
    "def get_html(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Функция для парсинга данных новости\n",
    "def parse_news(news_item):\n",
    "    news_link = 'https://nvinder.ru' + news_item.find('h3', class_='field-content').find('a')['href']\n",
    "\n",
    "    # Найти ссылку \"Читать далее...\"\n",
    "    read_more_link = 'https://nvinder.ru' + news_item.find('div', class_='readmore').find('a')['href']\n",
    "\n",
    "    # Получаем HTML-код страницы новости\n",
    "    news_page_html = get_html(read_more_link)\n",
    "\n",
    "    if news_page_html:\n",
    "        # Парсим HTML-код страницы новости\n",
    "        news_page_soup = BeautifulSoup(news_page_html, 'html.parser')\n",
    "\n",
    "        # Ищем контейнер с классом \"article\" и все абзацы внутри него\n",
    "        article_container = news_page_soup.find('article')\n",
    "        if article_container:\n",
    "            paragraphs = article_container.find_all('p')\n",
    "\n",
    "            # Инициализируем переменные для текста на разных языках и флаг для пропуска первого предложения\n",
    "            example_ru = \"\"\n",
    "            example_nn = \"\"\n",
    "            switch_language = False\n",
    "            skip_first_sentence = False\n",
    "\n",
    "            # Извлекаем текст из каждого абзаца\n",
    "            for paragraph in paragraphs:\n",
    "                if \"Перевод на ненецкий язык\" in paragraph.get_text():\n",
    "                    switch_language = True\n",
    "                    skip_first_sentence = True\n",
    "                    continue\n",
    "                elif \"Перевод на русский язык\" in paragraph.get_text():\n",
    "                    switch_language = False\n",
    "                    skip_first_sentence = True\n",
    "                    continue\n",
    "\n",
    "                # Пропускаем первое предложение, если флаг установлен\n",
    "                if skip_first_sentence:\n",
    "                    skip_first_sentence = False\n",
    "                    continue\n",
    "\n",
    "                # Добавляем текст в соответствующий столбец, добавляя точку в конце, если необходимо\n",
    "                text = paragraph.get_text().strip()\n",
    "                if text:\n",
    "                    if text[-1] not in [\".\", \"!\", \"?\"]:\n",
    "                        text += \".\"\n",
    "                    if switch_language:\n",
    "                        example_nn += text + \"\\n\"\n",
    "                    else:\n",
    "                        example_ru += text + \"\\n\"\n",
    "\n",
    "            # Если оба столбца не пусты, добавляем данные в список\n",
    "            if example_ru.strip() and example_nn.strip():\n",
    "                return {'example_ru': example_ru.strip(), 'example_nn': example_nn.strip(), 'source': news_link}\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Список для хранения данных о новостях\n",
    "news_data = []\n",
    "\n",
    "# Парсинг каждой указанной страницы и получение данных о новостях\n",
    "for page_num in range(0, 1): # Необходимо указать требуемое количество страниц\n",
    "    url = f'https://nvinder.ru/rubric/yalumd?page={page_num}'\n",
    "    html = get_html(url)\n",
    "    if html:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        news_items = soup.find_all('div', class_='views-row')\n",
    "        for news_item in news_items:\n",
    "            parsed_news = parse_news(news_item)\n",
    "            if parsed_news:\n",
    "                news_data.append(parsed_news)\n",
    "\n",
    "# Создание DataFrame для хранения данных\n",
    "df = pd.DataFrame(news_data)\n",
    "\n",
    "# Вывод первых нескольких строк DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Сохранение данных в CSV файл\n",
    "df.to_csv('news_data.csv', index=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7mnmqnKQXyh",
    "outputId": "a3412359-c17e-447b-f368-7698ecf5b5dd"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                          example_ru  \\\n",
      "0  В последний день января стартовал новый проект...   \n",
      "1  Рассказываем о потомственных оленеводах из Инд...   \n",
      "2  В этом году Институт народов Севера РГПУ имени...   \n",
      "3  В Нарьян-Маре подвели итоги конкурса «Семейно-...   \n",
      "4  В Ненецкой окружной библиотеке прошёл краеведч...   \n",
      "\n",
      "                                          example_nn  \\\n",
      "0  Хаювы ирий’ пудана яляхана этнокультурной цент...   \n",
      "1  Тюку яля’ вадими’ Индига’ ӈэсы’ тер’’ тыбэрма’...   \n",
      "2  Тюку по’ А.И.Герцен’ нювм’ нюбета РГПУ’ ӈэрм’ ...   \n",
      "3  Тюку по’ нертей яля’’ пиркана Нарьяна маркы’ а...   \n",
      "4  Тюку по’ я’ сяр нина’’ илена’’ хибяри’’ ханяри...   \n",
      "\n",
      "                                              source  \n",
      "0  https://nvinder.ru/article/vypusk-no-12-21642-...  \n",
      "1  https://nvinder.ru/article/vypusk-no-10-21640-...  \n",
      "2  https://nvinder.ru/article/vypusk-no-9-21639-o...  \n",
      "3  https://nvinder.ru/article/vypusk-no-7-21637-o...  \n",
      "4  https://nvinder.ru/article/vypusk-no-6-21636-o...  \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Загрузка данных из CSV файла в датафрейм\n",
    "df = pd.read_csv('news_data.csv')\n",
    "\n",
    "# Функция для разделения текста на предложения\n",
    "def split_text_into_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "# Создание нового датафрейма для разделенных записей\n",
    "split_news_data = pd.DataFrame(columns=['source', 'example_ru', 'example_nn'])\n",
    "\n",
    "# Создание нового датафрейма для записей, которые не удалось разделить\n",
    "row_news_data = pd.DataFrame(columns=['source', 'example_ru', 'example_nn'])\n",
    "\n",
    "# Перебор всех записей в исходном датафрейме\n",
    "for idx, row in df.iterrows():\n",
    "    example_ru_sentences = split_text_into_sentences(row['example_ru'])\n",
    "    example_nn_sentences = split_text_into_sentences(row['example_nn'])\n",
    "\n",
    "    # Проверка на равенство количества предложений\n",
    "    if len(example_ru_sentences) == len(example_nn_sentences):\n",
    "        # Добавление разделенных записей в новый датафрейм\n",
    "        for i in range(len(example_ru_sentences)):\n",
    "            split_news_data = pd.concat([split_news_data, pd.DataFrame({\n",
    "                'source': row['source'],\n",
    "                'example_ru': [example_ru_sentences[i]],\n",
    "                'example_nn': [example_nn_sentences[i]]\n",
    "            })], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"Index {idx} has unequal number of sentences\")\n",
    "        # Добавление исходной записи в новый датафрейм\n",
    "        row_news_data = pd.concat([row_news_data, pd.DataFrame({\n",
    "            'source': [row['source']],\n",
    "            'example_ru': [row['example_ru']],\n",
    "            'example_nn': [row['example_nn']]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "# Сохранение разделенных записей в CSV файл\n",
    "split_news_data.to_csv('split_news_data.csv', index=False)\n",
    "\n",
    "# Сохранение записей с неравным количеством предложений в CSV файл\n",
    "row_news_data.to_csv('row_news_data.csv', index=False)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9gRZMpqGEb_",
    "outputId": "aa94cf05-b1da-48d3-fc3b-6fb0dc95ca86"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index 0 has unequal number of sentences\n",
      "Index 1 has unequal number of sentences\n",
      "Index 2 has unequal number of sentences\n",
      "Index 3 has unequal number of sentences\n",
      "Index 4 has unequal number of sentences\n",
      "Index 5 has unequal number of sentences\n",
      "Index 6 has unequal number of sentences\n",
      "Index 7 has unequal number of sentences\n",
      "Index 8 has unequal number of sentences\n",
      "Index 9 has unequal number of sentences\n",
      "Index 11 has unequal number of sentences\n",
      "Index 12 has unequal number of sentences\n",
      "Index 13 has unequal number of sentences\n",
      "Index 14 has unequal number of sentences\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Функция для подсчета количества предложений в тексте\n",
    "def count_sentences(text):\n",
    "    if pd.isna(text):  # Проверка на NaN\n",
    "        return 0\n",
    "    return len(sent_tokenize(text))\n",
    "\n",
    "# Функция для подсчета количества предложений в каждой записи\n",
    "def count_sentences_per_record(row):\n",
    "    count_ru_sentences = count_sentences(row['example_ru'])\n",
    "    count_nn_sentences = count_sentences(row['example_nn'])\n",
    "    return pd.Series({'sentences_ru': count_ru_sentences, 'sentences_nn': count_nn_sentences})\n",
    "\n",
    "# Создание новых столбцов с количеством предложений для каждой записи\n",
    "sentence_counts = df.apply(count_sentences_per_record, axis=1)\n",
    "\n",
    "# Объединение результатов с исходным датафреймом\n",
    "df_with_sentence_counts = pd.concat([df, sentence_counts], axis=1)\n",
    "\n",
    "# Вывод количества предложений в красивом формате\n",
    "for idx, row in df_with_sentence_counts.iterrows():\n",
    "    print(f\"Запись {idx}:\")\n",
    "    print(f\"Количество предложений в столбце 'example_ru': {row['sentences_ru']}\")\n",
    "    print(f\"Количество предложений в столбце 'example_nn': {row['sentences_nn']}\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJRR2_WR6uT2",
    "outputId": "8e4da990-d0ee-41a6-a00a-eb32443c02d2"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Запись 0:\n",
      "Количество предложений в столбце 'example_ru': 37\n",
      "Количество предложений в столбце 'example_nn': 45\n",
      "\n",
      "Запись 1:\n",
      "Количество предложений в столбце 'example_ru': 43\n",
      "Количество предложений в столбце 'example_nn': 44\n",
      "\n",
      "Запись 2:\n",
      "Количество предложений в столбце 'example_ru': 39\n",
      "Количество предложений в столбце 'example_nn': 37\n",
      "\n",
      "Запись 3:\n",
      "Количество предложений в столбце 'example_ru': 40\n",
      "Количество предложений в столбце 'example_nn': 46\n",
      "\n",
      "Запись 4:\n",
      "Количество предложений в столбце 'example_ru': 33\n",
      "Количество предложений в столбце 'example_nn': 35\n",
      "\n",
      "Запись 5:\n",
      "Количество предложений в столбце 'example_ru': 34\n",
      "Количество предложений в столбце 'example_nn': 44\n",
      "\n",
      "Запись 6:\n",
      "Количество предложений в столбце 'example_ru': 35\n",
      "Количество предложений в столбце 'example_nn': 41\n",
      "\n",
      "Запись 7:\n",
      "Количество предложений в столбце 'example_ru': 36\n",
      "Количество предложений в столбце 'example_nn': 43\n",
      "\n",
      "Запись 8:\n",
      "Количество предложений в столбце 'example_ru': 22\n",
      "Количество предложений в столбце 'example_nn': 28\n",
      "\n",
      "Запись 9:\n",
      "Количество предложений в столбце 'example_ru': 38\n",
      "Количество предложений в столбце 'example_nn': 44\n",
      "\n",
      "Запись 10:\n",
      "Количество предложений в столбце 'example_ru': 43\n",
      "Количество предложений в столбце 'example_nn': 43\n",
      "\n",
      "Запись 11:\n",
      "Количество предложений в столбце 'example_ru': 30\n",
      "Количество предложений в столбце 'example_nn': 28\n",
      "\n",
      "Запись 12:\n",
      "Количество предложений в столбце 'example_ru': 24\n",
      "Количество предложений в столбце 'example_nn': 30\n",
      "\n",
      "Запись 13:\n",
      "Количество предложений в столбце 'example_ru': 26\n",
      "Количество предложений в столбце 'example_nn': 30\n",
      "\n",
      "Запись 14:\n",
      "Количество предложений в столбце 'example_ru': 33\n",
      "Количество предложений в столбце 'example_nn': 38\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Эмпирически установленный факт частого расхождения в количестве предложений между русскоязычными и ненецкоязычными текстами указывает на возможные проблемы с качеством перевода. Это несоответствие может свидетельствовать о пропущенных, упрощенных или измененных фрагментах, что снижает надежность данных для обучения модели нейронного машинного перевода.\n",
    "\n",
    "Для более обоснованных выводов необходимо провести качественный анализ перевода, например, выборочную проверку соответствия оригинального и переведенного текстов. Поскольку для ненецкого языка не существуют готовые решения в области машинного перевода, в том числе автоматизированные метрики качества (такие как BLEU, TER или METEOR), альтернативным подходом может стать экспертная оценка и частотный анализ текстов."
   ],
   "metadata": {
    "id": "OCFPAtfGAD6y"
   }
  }
 ]
}
